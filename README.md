# local-deploy-Mid-size_LLM-qwen2-7b
A step-by-step, beginner-friendly guide to locally deploy and run free mid-size LLMs, using Qwen2-7B as an example. This guide is specifically tailored for deployment on an NVIDIA RTX 3090 .More information will be coming soon.

Prerequisitesï¼š
  System: Ubuntu 20.04/22.04 LTS (64-bit)

  Hardware: RTX 3090 (24GB VRAM, compatible with other NVIDIA GPUs)

  Base Environment: NVIDIA drivers + CUDA 11.8+ installed (verifiable via nvidia-smi)
